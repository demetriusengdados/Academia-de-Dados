{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "empresa1 = spark.createDataFrame(\n",
    "    [\n",
    "        (\"Joao Santos\", 2000),\n",
    "        (\"Carlos Fernandez\", 3400)\n",
    "    ], [\"Funcionario\", \"Salario\"])\n",
    "display(empresa1)\n",
    "empresa1.printSchema()\n",
    "\n",
    "#Setando o local de armazenamento dos arquivos parquet \n",
    "\n",
    "%python\n",
    "parquetpath = \"dbfs://FileStore/tables/delta/schema_evolution/parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setando o local de armazenamento dos arquivos parquet \n",
    "\n",
    "%python\n",
    "parquetpath = \"dbfs://FileStore/tables/delta/schema_evolution/parquet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando arquivos parquet com base no primeiro dataframe\n",
    "\n",
    "%python\n",
    "(\n",
    "    empresa1\n",
    "        .write\n",
    "        .format('parquet')\n",
    "        .save('/FileStore/tables/delta/schema_evolution/parquet')     \n",
    ")\n",
    "spark.read.parquet(oarquetpath).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando o segundo dataframe com dados, acrescentando novos campos(Setor, comissao)\n",
    "\n",
    "%python\n",
    "\n",
    "empresa2 = spark.createDataFrame(\n",
    "    [\n",
    "        (\"Financeiro\", 240),\n",
    "        (\"Marketing\", 540),\n",
    "    ], ['Setor', 'Comissao'])\n",
    "display(empresa2)\n",
    "empresa2.printschema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apesar de colocar \"append\" no parquet, não houve evolução do esquema, as colunas foram substituidas\n",
    "\n",
    "%python\n",
    "empresa2.write.mode(\"append\").parquet(parquetpath)\n",
    "spark.read.parquet(parquetpath).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos gerar o Schema Evolution com as tabelas delta\n",
    "\n",
    "%python\n",
    "deltapath = \"/FileStore/tables/delta/schema_evolution/delta\"\n",
    "(\n",
    "    empresa1\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .save(\"/FileStore/tables/delta/schema_evolution/delta\")\n",
    ")\n",
    "spark.read.format(\"delta\").load(deltapath).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos realizar um merge entre os dataframes, note que agora conseguirá realizar a junção entre os schemas. Os dados inexistentes foram acrescidos de nulos. \n",
    "\n",
    "%python\n",
    "(\n",
    "    empresa2\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode('append')\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .save(deltapath)\n",
    ")\n",
    "spark.read.format(\"delta\").load(deltapath).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "empresa3 = spark.createDataFrame(\n",
    "    [\n",
    "        (\"Sandra Lemos\", 672),\n",
    "        (\"Carla Soares\", 966),\n",
    "    ],\n",
    "    [\"Funcionário\", \"Comissao\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos acrescentar dados mais dados e verificar a inclusao apenas de alguns campos\n",
    "\n",
    "%Python\n",
    "(\n",
    "    empresa3\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .save(deltapath)\n",
    ")\n",
    "spark.read.format(\"delta\").load(deltapath).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos sobreescrever toda a tabela, perceba as mudanças dos campos que ficaram e dos registros. \"option=overwriteschema, mode=overwrite\"\n",
    "\n",
    "%python\n",
    "(\n",
    "    empresa3\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .save(deltapath)\n",
    ")\n",
    "spark.read.format(\"delta\").load(deltapath).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos criar uma tabela delta com referencia aos parquet(delta) criados\n",
    "\n",
    "%sql\n",
    "CREATE TABLE tab_empresa(\n",
    "    Funcionario STRING,\n",
    "    Comissao long\n",
    "    )\n",
    "USING delta\n",
    "LOCATION \"/FileStore/tables/delta/schema_evolution/delta\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos listar o historico gerado de todas as nossas mudanças\n",
    "\n",
    "%sql\n",
    "DESCRIBE HISTORY '/FileStore/tables/delta/schema_evolution/delta'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listando todas as versões que podemos utilizar\n",
    "\n",
    "%sql\n",
    "SELECT * FROM delta. \"/FileStore/tables/delta/schema_evolution/delta\" VERSION AS OF 4"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
